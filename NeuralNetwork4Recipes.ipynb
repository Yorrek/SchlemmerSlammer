{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vecRecipes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsiOhyaR98JE"
      },
      "source": [
        "# SchlemmerSlammer - Neural Network for recipes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jupWAxOvAWb1"
      },
      "source": [
        "## 1. Teammembers\n",
        "\n",
        "### Maxim Bex\n",
        "research, documentation, concept, coding\n",
        "\n",
        "### Hannes Gelbhardt\n",
        "coding, research, concept, documentation \n",
        "\n",
        "### York Smeddinck\n",
        "documentation, concept, coding, research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9hj9vEYgAeby"
      },
      "source": [
        "## 2. Problem Description\n",
        "\n",
        "We thought about training an AI to recommend / create recipes based on given attributes (e.g. rating, certain ingredients (include / exclude), calories, sodium, fat, FODMAP, etc.pp.).\n",
        "The following graphic is an entity relationship diagram for recipes, which we created to get a better understanding of the nature and complexity of our problem.\n",
        "\n",
        "![Recipe Entity Relation Diagram](https://i.imgur.com/PGj3fdZ.png)\n",
        "\n",
        "Since this would be a rather complex system to begin with, we decided to break the problem down, and start with a smaller application first.\n",
        "Thus we are trying to train an AI to recommend recipe ingredients based on given ingredients.\n",
        "\n",
        "Our rough plan is to use Word2Vec to put the content of the directions of the recipes in context, then train a model based on this data, and finally visualizing our results using T-SNE.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaGKHJwtH6V3",
        "colab_type": "text"
      },
      "source": [
        "## 3. Dataset description\n",
        "### Source: https://www.kaggle.com/hugodarwood/epirecipes\n",
        "The Dataset is a collection of about 21000 recipes given in a .csv and .json format, where the json file contained all information of the recipes and the .csv file is basically a list of attributes and ingredients for all recipes.\n",
        "We opted for using the json file, which includes more context of the ingredients (e.g. 3 evenly chopped tomatoes) compared to the csv file (e.g. 3 tomatoes), which should result in a more complex model.\n",
        "### Processing the data:\n",
        "Because we put our focus on the relationship of ingredients, we preprocess the data from the json format into a textfile consisting of single line listings of the recipes ingredients.\n",
        "#### Problems during preprocessing\n",
        "The first problem was that the windows command line could not process specific characters of the dataset to a file, which first was solved by setting the command line charset to \"UTF - 8\"\n",
        "Unfortunetly we just could read and process about 700 more recipes, but not all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqDoZh-qUZmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "badchars = set(\"(),-*\\\"'<>|:\")\n",
        "\n",
        "with open('full_format_recipes.json') as json_file:  \n",
        "\tdata = json.load(json_file)\n",
        "\toutFile = \"./preProcessedData.txt\"\n",
        "\twith open (outFile, \"w\") as f:\n",
        "\t\tfor recipe in data:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tingredients = recipe['ingredients']\n",
        "\t\t\t\tfor ingredient in ingredients:\n",
        "\t\t\t\t\tfor c in badchars:\n",
        "\t\t\t\t\t\tingredient = ingredient.replace(c,' ')\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tf.write(ingredient+\" \")\n",
        "\t\t\t\t\texcept UnicodeEncodeError:\n",
        "\t\t\t\t\t\tprint(\"UnicodeEncodeError\")\n",
        "\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\texcept KeyError:\n",
        "        print(\"KeyError\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaVQn7pvVD-7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J8nMYwIU-fib",
        "colab": {}
      },
      "source": [
        "import d2l\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "import pandas as pd\n",
        "\n",
        "import collections\n",
        "import math\n",
        "from mxnet import autograd, gluon, nd\n",
        "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
        "import random\n",
        "import sys\n",
        "import collections\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gab4xUBg-ZTK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VEjFCkOnyb4v",
        "outputId": "566b11ee-c05d-49d0-cc61-bef77f7054f2",
        "colab": {}
      },
      "source": [
        "#read the input data as line = sentence and print the number of sentences\n",
        "with open('./data.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    # st is the abbreviation of \"sentence\" in the loop\n",
        "    ingredients = [st.split() for st in lines]\n",
        "\n",
        "'# sentences: %d' % len(ingredients)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# sentences: 1076'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RW4t0nynyb43",
        "outputId": "2738e5f8-3d46-416a-c226-48c88923de13",
        "colab": {}
      },
      "source": [
        "#For test purposes print tge first 3 lines with the first 5 \"words\"\n",
        "for st in ingredients [:3]:\n",
        "    print('# tokens:', len(st), st[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# tokens: 90 [\"['4\", 'cups', 'low-sodium', 'vegetable', 'or']\n",
            "# tokens: 123 [\"['1\", '1/2', 'cups', 'whipping', \"cream',\"]\n",
            "# tokens: 38 [\"['1\", 'fennel', 'bulb', '(sometimes', 'called']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DiPGI7jSyb49",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# tk is an abbreviation for \"token\" in the loop\n",
        "counter = collections.Counter([tk for st in ingredients for tk in st])\n",
        "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "25k2tJLqyb5B",
        "outputId": "9e324b11-efb1-43cb-ea19-329b1e2ce668",
        "colab": {}
      },
      "source": [
        "#count the total number of tokens in the dataset\n",
        "idx_to_token = [tk for tk, _ in counter.items()]\n",
        "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
        "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
        "           for st in ingredients]\n",
        "num_tokens = sum([len(st) for st in dataset])\n",
        "'# tokens: %d' % num_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# tokens: 56124'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ab8F1W92yb5G",
        "outputId": "f0d3e3cc-2db2-4b44-c104-0e6988922c39",
        "colab": {}
      },
      "source": [
        "#Subsampling -> normalization of the word relation. \n",
        "# Remove Tokens? \n",
        "def discard(idx):\n",
        "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
        "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
        "\n",
        "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
        "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# tokens: 14761'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9-gHwvWAyb5L",
        "outputId": "1836fb46-dc27-4821-b375-f348cfcd200a",
        "colab": {}
      },
      "source": [
        "\n",
        "#Wordcount before and after subsampling of the word \"cup\"\n",
        "def compare_counts(token):\n",
        "    return '# %s: before=%d, after=%d' % (token, sum(\n",
        "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
        "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
        "\n",
        "compare_counts('cup')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# cup: before=2259, after=112'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtQgmd6uyb5Q",
        "colab": {}
      },
      "source": [
        "#Words with a distance from the target window that not exceeding the max window size\n",
        "#Is there a need?\n",
        "def get_centers_and_contexts(dataset, max_window_size):\n",
        "    centers, contexts = [], []\n",
        "    for st in dataset:\n",
        "        # Each sentence needs at least 2 words to form a\n",
        "        # \"central target word - context word\" pair\n",
        "        if len(st) < 2:\n",
        "            continue\n",
        "        centers += st\n",
        "        for center_i in range(len(st)):\n",
        "            window_size = random.randint(1, max_window_size)\n",
        "            indices = list(range(max(0, center_i - window_size),\n",
        "                                 min(len(st), center_i + 1 + window_size)))\n",
        "            # Exclude the central target word from the context words\n",
        "            indices.remove(center_i)\n",
        "            contexts.append([st[idx] for idx in indices])\n",
        "    return centers, contexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKYVWXmDyb5U",
        "outputId": "cc4a9ea8-c233-445b-f3eb-e611b63fd1c4",
        "colab": {}
      },
      "source": [
        "#Create artificilly a dataset with two random sentences of 2 to 7 words each\n",
        "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
        "print('dataset', tiny_dataset)\n",
        "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
        "    print('center', center, 'has contexts', context)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
            "center 0 has contexts [1]\n",
            "center 1 has contexts [0, 2]\n",
            "center 2 has contexts [1, 3]\n",
            "center 3 has contexts [2, 4]\n",
            "center 4 has contexts [2, 3, 5, 6]\n",
            "center 5 has contexts [3, 4, 6]\n",
            "center 6 has contexts [4, 5]\n",
            "center 7 has contexts [8]\n",
            "center 8 has contexts [7, 9]\n",
            "center 9 has contexts [7, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSWf_7QFyb5b",
        "colab": {}
      },
      "source": [
        "#test example, extracting words to the context max window size of 5\n",
        "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NM92f-Jpyb5f",
        "colab": {}
      },
      "source": [
        "#Negative sampling for training\n",
        "def get_negatives(all_contexts, sampling_weights, K):\n",
        "    all_negatives, neg_candidates, i = [], [], 0\n",
        "    population = list(range(len(sampling_weights)))\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            if i == len(neg_candidates):\n",
        "                # An index of k words is randomly generated as noise words\n",
        "                # based on the weight of each word (sampling_weights). For\n",
        "                # efficient calculation, k can be set slightly larger\n",
        "                i, neg_candidates = 0, random.choices(\n",
        "                    population, sampling_weights, k=int(1e5))\n",
        "            neg, i = neg_candidates[i], i + 1\n",
        "            # Noise words cannot be context words\n",
        "            if neg not in set(contexts):\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
        "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0LpW_kwAyb5j",
        "colab": {}
      },
      "source": [
        "#using tiny batches for the data reading process in a function\n",
        "def batchify(data):\n",
        "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "    for center, context, negative in data:\n",
        "        cur_len = len(context) + len(negative)\n",
        "        centers += [center]\n",
        "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "    return (nd.array(centers).reshape((-1, 1)), nd.array(contexts_negatives),\n",
        "            nd.array(masks), nd.array(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zrg5qLnuyb5n",
        "outputId": "cd602905-7acc-4d95-f588-50094bea8e11",
        "colab": {}
      },
      "source": [
        "#We use the previously defined batchify function to specify the data loader instance \n",
        "#and print the shape of each variable into the first batch read\n",
        "batch_size = 512\n",
        "#Checks how many cpu are available\n",
        "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "dataset = gdata.ArrayDataset(all_centers, all_contexts, all_negatives)\n",
        "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True,\n",
        "                             batchify_fn=batchify, num_workers=num_workers)\n",
        "#change of zip?\n",
        "for batch in data_iter:\n",
        "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
        "                           'labels'], batch):\n",
        "        print(name, 'shape:', data.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "centers shape: (512, 1)\n",
            "contexts_negatives shape: (512, 60)\n",
            "masks shape: (512, 60)\n",
            "labels shape: (512, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_saMreEJyb5s",
        "outputId": "8d2b5432-7d70-4e01-c11f-3c48d1032a75",
        "colab": {}
      },
      "source": [
        "#Skip-Gram Model\n",
        "#Embedding a layer\n",
        "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
        "embed.initialize()\n",
        "embed.weight"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter embedding0_weight (shape=(20, 4), dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6sCbi-Gyb5x",
        "outputId": "29b88aa0-8cca-476e-f95f-3378da80da4f",
        "colab": {}
      },
      "source": [
        "#The input of the embedding layer is the index of the context word\n",
        "x = nd.array([[1, 2, 3], [4, 5, 6]])\n",
        "embed(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[[ 0.01438687  0.05011239  0.00628365  0.04861524]\n",
              "  [-0.01068833  0.01729892  0.02042518 -0.01618656]\n",
              "  [-0.00873779 -0.02834515  0.05484822 -0.06206018]]\n",
              "\n",
              " [[ 0.06491279 -0.03182812 -0.01631819 -0.00312688]\n",
              "  [ 0.0408415   0.04370362  0.00404529 -0.0028032 ]\n",
              "  [ 0.00952624 -0.01501013  0.05958354  0.04705103]]]\n",
              "<NDArray 2x3x4 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bs82zfY0yb57",
        "outputId": "1535ed5f-e8fd-4372-fd46-dc5dcb274944",
        "colab": {}
      },
      "source": [
        "#Mini Batch Multiplication\n",
        "X = nd.ones((2, 1, 4))\n",
        "Y = nd.ones((2, 4, 6))\n",
        "nd.batch_dot(X, Y).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZfgDsA4Myb6F",
        "colab": {}
      },
      "source": [
        "#Skip-Gram Forward Calculation\n",
        "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
        "    v = embed_v(center)\n",
        "    u = embed_u(contexts_and_negatives)\n",
        "    pred = nd.batch_dot(v, u.swapaxes(1, 2))\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x-4NF2royb6J",
        "colab": {}
      },
      "source": [
        "#Train a Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L90AWu0nyb6M",
        "colab": {}
      },
      "source": [
        "#Gluon's binary cross entropy loss function SigmoidBinaryCrossEntropyLoss\n",
        "loss = gloss.SigmoidBinaryCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yjrY5pFAyb6P",
        "outputId": "fc5351ca-8724-4ecc-8d8b-069b593dfc08",
        "colab": {}
      },
      "source": [
        "#Mask functions are considerable\n",
        "pred = nd.array([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
        "# 1 and 0 in the label variables label represent context words and the noise\n",
        "# words, respectively\n",
        "label = nd.array([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
        "mask = nd.array([[1, 1, 1, 1], [1, 1, 1, 0]])  # Mask variable\n",
        "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[0.8739896 1.2099689]\n",
              "<NDArray 2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUsLgK-0yb6Y",
        "outputId": "463fcade-c1cc-406b-e70f-2acec2d4c995",
        "colab": {}
      },
      "source": [
        "#binary cross-entropy loss function calculation to compare\n",
        "#and calculate the predicted value with a mask of 1 and the loss of the label based on the mask variable mask.\n",
        "def sigmd(x):\n",
        "    return -math.log(1 / (1 + math.exp(-x)))\n",
        "\n",
        "print('%.7f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4))\n",
        "print('%.7f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8739896\n",
            "1.2099689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y9zlPI0kyb6d",
        "colab": {}
      },
      "source": [
        "#Initilize Model Parameter\n",
        "embed_size = 100\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size),\n",
        "        nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mSA6tnW_yb6j",
        "colab": {}
      },
      "source": [
        "#Training function\n",
        "def train(net, lr, num_epochs):\n",
        "    ctx = d2l.try_gpu()\n",
        "    net.initialize(ctx=ctx, force_reinit=True)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    for epoch in range(num_epochs):\n",
        "        start, l_sum, n = time.time(), 0.0, 0\n",
        "        for batch in data_iter:\n",
        "            center, context_negative, mask, label = [\n",
        "                data.as_in_context(ctx) for data in batch]\n",
        "            with autograd.record():\n",
        "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
        "                # Use the mask variable to avoid the effect of padding on loss\n",
        "                # function calculations\n",
        "                l = (loss(pred.reshape(label.shape), label, mask) *\n",
        "                     mask.shape[1] / mask.sum(axis=1))\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            l_sum += l.sum().asscalar()\n",
        "            n += l.size\n",
        "        print('epoch %d, loss %.2f, time %.2fs'\n",
        "              % (epoch + 1, l_sum / n, time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S0E6j1yZyb6o",
        "outputId": "15842b3e-8466-4ccb-c3a4-b220406bc517",
        "colab": {}
      },
      "source": [
        "train(net, 0.005, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.67, time 15.31s\n",
            "epoch 2, loss 0.48, time 15.15s\n",
            "epoch 3, loss 0.43, time 15.30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIdfQsXqyb6t",
        "outputId": "4f63dc79-869e-45ce-f7b3-987cc02d719f",
        "colab": {}
      },
      "source": [
        "#Applying word embedding model\n",
        "def get_similar_tokens(query_token, k, embed):\n",
        "    W = embed.weight.data()\n",
        "    x = W[token_to_idx[query_token]]\n",
        "    # The added 1e-9 is for numerical stability\n",
        "    cos = nd.dot(W, x) / (nd.sum(W * W, axis=1) * nd.sum(x * x) + 1e-9).sqrt()\n",
        "    topk = nd.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
        "    for i in topk[1:]:  # Remove the input words\n",
        "        print('cosine sim=%.3f: %s' % (cos[i].asscalar(), (idx_to_token[i])))\n",
        "\n",
        "get_similar_tokens('toasted', 10, net[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine sim=0.863: 'Vegetable\n",
            "cosine sim=0.860: halves\n",
            "cosine sim=0.860: removed\n",
            "cosine sim=0.858: top\n",
            "cosine sim=0.858: jigger)\n",
            "cosine sim=0.853: ginger']\n",
            "cosine sim=0.849: any\n",
            "cosine sim=0.849: springform\n",
            "cosine sim=0.848: melted,\n",
            "cosine sim=0.845: sprig\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1m2D_geXyb6x",
        "outputId": "b4006b52-a8f5-48d6-e1b7-c060ba907e8c",
        "colab": {}
      },
      "source": [
        "glove_file = datapath('D:\\Studium\\DeepLearning\\Scripte\\output3.txt')\n",
        "glove_file = 'D:\\Studium\\DeepLearning\\Scripte\\output3.txt'\n",
        "\n",
        "word2vec_glove_file = get_tmpfile(\"output3.txt\")\n",
        "word2vec_glove_file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\Gazt\\\\AppData\\\\Local\\\\Temp\\\\output3.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Me95GjNTyb63",
        "outputId": "82fd2d91-f11f-4fc9-a595-44a86a0bceca",
        "colab": {}
      },
      "source": [
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1076, 89)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kzza_N0myb68",
        "outputId": "bc64b60f-d3e5-4458-a5e5-0e3cb3fb5dbf",
        "colab": {}
      },
      "source": [
        "#model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'cups'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-30-b0467350a93d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_glove_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mD:\\ProgramFiles\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1475\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\ProgramFiles\\Anaconda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                 \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\ProgramFiles\\Anaconda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                 \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'cups'"
          ]
        }
      ]
    }
  ]
}