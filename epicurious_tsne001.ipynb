{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epicurious_tsne.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yorrek/SchlemmerSlammer/blob/master/epicurious_tsne001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm-SNvai7LjH",
        "colab_type": "text"
      },
      "source": [
        "MAT 259: Final Project\n",
        "\n",
        "Chantal Nguyen\n",
        "\n",
        "Instructor: George Legrady\n",
        "\n",
        "\n",
        "The following code reads in a json file containing recipes extracted from the online recipe database Epicurious.com. The data includes recipe titles, ratings, review counts, dates of publication, ingredient lists, instructions, etc. \n",
        "\n",
        "tf-idf is used to vectorize the ingredient lists of each recipe. Dimensionality reduction is performed first with SVD and then with t-SNE to project recipes into 2D space. k-means clustering is used to cluster recipes into 15 clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4hPcTpi7LjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW78aRzn7Ljs",
        "colab_type": "text"
      },
      "source": [
        "Load in the json file as a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Nrvwst7Ljx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load in the data\n",
        "data = pd.read_json('epicurious-recipes.json',lines=True)\n",
        "data = data[data.reviewsCount > 0]\n",
        "for index, row in data.iterrows(): # clean up entries with no ingredients listed\n",
        "    if not isinstance(row['ingredients'],(list,)):\n",
        "        data = data.drop(index)\n",
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fhzXQWZ7LkD",
        "colab_type": "text"
      },
      "source": [
        "Extract ingredient lists and remove numbers and non-alphabetic characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRoTtIjN7LkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ingredients_data = []\n",
        "chars_to_remove = dict((ord(char), None) for char in '/0123456789!\"#%\\'()*+,-./:;<=>?@[\\]^_`{|}~')\n",
        "for index, row in data.iterrows():\n",
        "    ingredients_data.append((''.join(row['ingredients'])).translate(chars_to_remove))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4uNd9f7LkV",
        "colab_type": "text"
      },
      "source": [
        "Define stopwords: common English stopwords (found here: https://github.com/stopwords-iso/stopwords-en/) plus words related to measurement quantities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPVhRyZz7LkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('stopwords-en.json') as f:\n",
        "    stopwords = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIeV6ll7Lkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = stopwords + [u'cup', u'cups', u'tablespoon', u'tablespoons', u'teaspoon', u'teaspoons', u'c', u'tbsp', u'tsp', u'oz', u'g', u'kg', u'lb', u'pt', u'gal', u'qt', u'qts', u'tbsps', u'tsps', u'ml', u'l', u'inch', u'inches', u'pinch', u'pinches', u'dash', u'dashes', u'ounce', u'ounces', u'can', u'cans', u'bag', u'bags', u'package', u'packages', u'gram', u'grams', u'pound', u'pounds']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRZQ5YKS7Lkn",
        "colab_type": "text"
      },
      "source": [
        "Vectorize with tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtWpHARj7Lks",
        "colab_type": "code",
        "outputId": "aa18a9fd-5a6a-472e-f69e-d2db6dcf5160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
        "                                 min_df=2, stop_words=stopwords,\n",
        "                                 use_idf=True)\n",
        "X = vectorizer.fit_transform(ingredients_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [u'ain', u'daren', u'hadn', u'herse', u'himse', u'itse', u'mayn', u'mightn', u'mon', u'mustn', u'myse', u'needn', u'oughtn', u'shan'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BMT7QSP7Lkz",
        "colab_type": "text"
      },
      "source": [
        "First reduce dimensionality to 50 dimensions with SVD (performing t-SNE now would be too intensive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk9MLldE7Lk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(n_components=50)\n",
        "Y = svd.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1SOF_9E7Lk6",
        "colab_type": "text"
      },
      "source": [
        "Reduce to 2D with t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFq_H7g97Lk9",
        "colab_type": "code",
        "outputId": "bfe96d42-b74a-4ee0-919d-2c1bbbf1116e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=1)\n",
        "Z = tsne_model.fit_transform(Y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 31417 samples in 0.084s...\n",
            "[t-SNE] Computed neighbors for 31417 samples in 116.772s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 6000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 7000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 8000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 9000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 10000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 11000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 12000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 13000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 14000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 15000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 16000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 17000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 18000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 19000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 20000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 21000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 22000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 23000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 24000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 25000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 26000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 27000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 28000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 29000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 30000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 31000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 31417 / 31417\n",
            "[t-SNE] Mean sigma: 0.114242\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 103.979744\n",
            "[t-SNE] KL divergence after 1000 iterations: 3.525492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h7h31Ln7LlI",
        "colab_type": "text"
      },
      "source": [
        "Extract t-SNE coordinates, append to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiMNlUri7LlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_coords = Z[:,0]\n",
        "y_coords = Z[:,1]\n",
        "coords = np.transpose(np.vstack((x_coords, y_coords)))  \n",
        "with open('epi_coords.csv', 'w') as f:\n",
        "    for x, y in coords:\n",
        "        f.write('%f, %f\\n' % (x,y)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWQnOUWM7LlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coords_df = pd.DataFrame(np.stack((x_coords,y_coords)).transpose(), columns=list('xy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9j--lx97LlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[['hed', 'pubDate', 'aggregateRating','reviewsCount','willMakeAgainPct']]\n",
        "data = data.join(coords_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SmcxiIV7LlZ",
        "colab_type": "text"
      },
      "source": [
        "Perform k-means clustering into (arbitrarily-set) 15 clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH7TMMqV7Lla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3087
        },
        "outputId": "2a25120b-0b69-44dc-f5f5-df28dcc5b506"
      },
      "source": [
        "num_clusters = 15\n",
        "km = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1)\n",
        "print(\"Clustering sparse data with %s\" % km)\n",
        "km.fit(X)\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d:\" % i)\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind])\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
            "    n_clusters=15, n_init=1, n_jobs=None, precompute_distances='auto',\n",
            "    random_state=None, tol=0.0001, verbose=0)\n",
            "Top terms per cluster:\n",
            "Cluster 0:\n",
            " chicken\n",
            " broth\n",
            " chopped\n",
            " lowsalt\n",
            " canned\n",
            " white\n",
            " dried\n",
            " oil\n",
            " butter\n",
            " garlic\n",
            "Cluster 1:\n",
            " sauce\n",
            " soy\n",
            " sesame\n",
            " rice\n",
            " asian\n",
            " minced\n",
            " oil\n",
            " ginger\n",
            " peeled\n",
            " garlic\n",
            "Cluster 2:\n",
            " baking\n",
            " flour\n",
            " sugar\n",
            " unsalted\n",
            " butter\n",
            " allpurpose\n",
            " powder\n",
            " salt\n",
            " soda\n",
            " vanilla\n",
            "Cluster 3:\n",
            " sugar\n",
            " water\n",
            " salt\n",
            " butter\n",
            " lime\n",
            " unsalted\n",
            " juice\n",
            " chopped\n",
            " cream\n",
            " equipment\n",
            "Cluster 4:\n",
            " lemon\n",
            " juice\n",
            " grated\n",
            " zest\n",
            " finely\n",
            " chopped\n",
            " sugar\n",
            " peel\n",
            " oil\n",
            " olive\n",
            "Cluster 5:\n",
            " chopped\n",
            " finely\n",
            " onion\n",
            " oil\n",
            " garlic\n",
            " pepper\n",
            " red\n",
            " olive\n",
            " tomatoes\n",
            " seeded\n",
            "Cluster 6:\n",
            " orange\n",
            " juice\n",
            " peel\n",
            " sugar\n",
            " grated\n",
            " zest\n",
            " lemon\n",
            " finely\n",
            " liqueur\n",
            " chopped\n",
            "Cluster 7:\n",
            " chocolate\n",
            " unsweetened\n",
            " bittersweet\n",
            " semisweet\n",
            " sugar\n",
            " cream\n",
            " vanilla\n",
            " chopped\n",
            " unsalted\n",
            " powder\n",
            "Cluster 8:\n",
            " sugar\n",
            " cream\n",
            " vanilla\n",
            " egg\n",
            " unsalted\n",
            " butter\n",
            " extract\n",
            " chilled\n",
            " whipping\n",
            " heavy\n",
            "Cluster 9:\n",
            " ground\n",
            " pepper\n",
            " black\n",
            " freshly\n",
            " salt\n",
            " chopped\n",
            " oil\n",
            " olive\n",
            " garlic\n",
            " cumin\n",
            "Cluster 10:\n",
            " cut\n",
            " pieces\n",
            " peeled\n",
            " potatoes\n",
            " chopped\n",
            " medium\n",
            " inchthick\n",
            " slices\n",
            " oil\n",
            " lengthwise\n",
            "Cluster 11:\n",
            " divided\n",
            " chopped\n",
            " oil\n",
            " olive\n",
            " ground\n",
            " kosher\n",
            " salt\n",
            " finely\n",
            " pepper\n",
            " garlic\n",
            "Cluster 12:\n",
            " cheese\n",
            " grated\n",
            " parmesan\n",
            " chopped\n",
            " freshly\n",
            " olive\n",
            " oil\n",
            " garlic\n",
            " pepper\n",
            " butter\n",
            "Cluster 13:\n",
            " olive\n",
            " oil\n",
            " vinegar\n",
            " extravirgin\n",
            " minced\n",
            " garlic\n",
            " chopped\n",
            " red\n",
            " leaves\n",
            " wine\n",
            "Cluster 14:\n",
            " sliced\n",
            " thinly\n",
            " red\n",
            " oil\n",
            " chopped\n",
            " olive\n",
            " vinegar\n",
            " pepper\n",
            " onion\n",
            " halved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIurt_ev7Llh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['label'] = km.labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KTILDk-7Lln",
        "colab_type": "text"
      },
      "source": [
        "Do some cleanup and save data as json file containing recipe title, rating, review count, publication date, % of reviewers who said they will make the recipe again, t-SNE coords, and cluster label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKge_fz-7Llo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datestrings = []\n",
        "for index, row in data.iterrows():\n",
        "    pdate = row['pubDate']\n",
        "    data.at[index,'pubDate'] = time.mktime(datetime.strptime(pdate, '%Y-%m-%dT%H:%M:%S.%fZ').timetuple())/1e+09\n",
        "    datestrings.append(pdate[0:10])\n",
        "data['datestr'] = datestrings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKD4KJRJ7Lls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('epicurious_data.json','w') as f:\n",
        "    f.write(data.to_json(orient = \"records\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0b_FVO7Llz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}