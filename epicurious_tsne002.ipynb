{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epicurious_tsne.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yorrek/SchlemmerSlammer/blob/master/epicurious_tsne002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm-SNvai7LjH",
        "colab_type": "text"
      },
      "source": [
        "MAT 259: Final Project\n",
        "\n",
        "Chantal Nguyen\n",
        "\n",
        "Instructor: George Legrady\n",
        "\n",
        "\n",
        "The following code reads in a json file containing recipes extracted from the online recipe database Epicurious.com. The data includes recipe titles, ratings, review counts, dates of publication, ingredient lists, instructions, etc. \n",
        "\n",
        "tf-idf is used to vectorize the ingredient lists of each recipe. Dimensionality reduction is performed first with SVD and then with t-SNE to project recipes into 2D space. k-means clustering is used to cluster recipes into 15 clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4hPcTpi7LjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW78aRzn7Ljs",
        "colab_type": "text"
      },
      "source": [
        "Load in the json file as a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Nrvwst7Ljx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load in the data\n",
        "data = pd.read_json('epicurious-recipes.json',lines=True)\n",
        "data = data[data.reviewsCount > 0]\n",
        "for index, row in data.iterrows(): # clean up entries with no ingredients listed\n",
        "    if not isinstance(row['ingredients'],(list,)):\n",
        "        data = data.drop(index)\n",
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fhzXQWZ7LkD",
        "colab_type": "text"
      },
      "source": [
        "Extract ingredient lists and remove numbers and non-alphabetic characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRoTtIjN7LkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ingredients_data = []\n",
        "chars_to_remove = dict((ord(char), None) for char in '/0123456789!\"#%\\'()*+,-./:;<=>?@[\\]^_`{|}~')\n",
        "for index, row in data.iterrows():\n",
        "    ingredients_data.append((''.join(row['ingredients'])).translate(chars_to_remove))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4uNd9f7LkV",
        "colab_type": "text"
      },
      "source": [
        "Define stopwords: common English stopwords (found here: https://github.com/stopwords-iso/stopwords-en/) plus words related to measurement quantities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPVhRyZz7LkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('stopwords-en.json') as f:\n",
        "    stopwords = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIeV6ll7Lkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = stopwords + [u'cup', u'cups', u'tablespoon', u'tablespoons', u'teaspoon', u'teaspoons', u'c', u'tbsp', u'tsp', u'oz', u'g', u'kg', u'lb', u'pt', u'gal', u'qt', u'qts', u'tbsps', u'tsps', u'ml', u'l', u'inch', u'inches', u'pinch', u'pinches', u'dash', u'dashes', u'ounce', u'ounces', u'can', u'cans', u'bag', u'bags', u'package', u'packages', u'gram', u'grams', u'pound', u'pounds']\n",
        "stopwords = stopwords + [u'purpose', u'allpurpose', u'salted', u'unsalted', u'extract', u'stick', u'temperature', u'powder',u'peeled']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRZQ5YKS7Lkn",
        "colab_type": "text"
      },
      "source": [
        "Vectorize with tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtWpHARj7Lks",
        "colab_type": "code",
        "outputId": "b51cefed-dc00-4419-d01e-fc82b42f2533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5, \n",
        "    max_features=10000,\n",
        "    min_df=2, \n",
        "    stop_words=stopwords,\n",
        "    use_idf=True\n",
        "    )\n",
        "X = vectorizer.fit_transform(ingredients_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [u'ain', u'daren', u'hadn', u'herse', u'himse', u'itse', u'mayn', u'mightn', u'mon', u'mustn', u'myse', u'needn', u'oughtn', u'shan'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BMT7QSP7Lkz",
        "colab_type": "text"
      },
      "source": [
        "First reduce dimensionality to 50 dimensions with SVD (performing t-SNE now would be too intensive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk9MLldE7Lk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(n_components=50)\n",
        "Y = svd.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1SOF_9E7Lk6",
        "colab_type": "text"
      },
      "source": [
        "Reduce to 2D with t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFq_H7g97Lk9",
        "colab_type": "code",
        "outputId": "dac75710-6fe1-4ed8-9249-3e72c5cec82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=1)\n",
        "Z = tsne_model.fit_transform(Y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 31417 samples in 0.083s...\n",
            "[t-SNE] Computed neighbors for 31417 samples in 117.918s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 6000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 7000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 8000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 9000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 10000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 11000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 12000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 13000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 14000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 15000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 16000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 17000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 18000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 19000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 20000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 21000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 22000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 23000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 24000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 25000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 26000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 27000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 28000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 29000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 30000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 31000 / 31417\n",
            "[t-SNE] Computed conditional probabilities for sample 31417 / 31417\n",
            "[t-SNE] Mean sigma: 0.114006\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 104.027832\n",
            "[t-SNE] KL divergence after 1000 iterations: 3.504326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h7h31Ln7LlI",
        "colab_type": "text"
      },
      "source": [
        "Extract t-SNE coordinates, append to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiMNlUri7LlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_coords = Z[:,0]\n",
        "y_coords = Z[:,1]\n",
        "coords = np.transpose(np.vstack((x_coords, y_coords)))  \n",
        "with open('epi_coords.csv', 'w') as f:\n",
        "    for x, y in coords:\n",
        "        f.write('%f, %f\\n' % (x,y)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWQnOUWM7LlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coords_df = pd.DataFrame(np.stack((x_coords,y_coords)).transpose(), columns=list('xy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9j--lx97LlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[['hed', 'pubDate', 'aggregateRating','reviewsCount','willMakeAgainPct']]\n",
        "data = data.join(coords_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SmcxiIV7LlZ",
        "colab_type": "text"
      },
      "source": [
        "Perform k-means clustering into (arbitrarily-set) 15 clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH7TMMqV7Lla",
        "colab_type": "code",
        "outputId": "1067fc28-0ca2-441c-ab3f-3353b89b122e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2488
        }
      },
      "source": [
        "num_clusters = 12\n",
        "km = KMeans(\n",
        "      n_clusters=num_clusters, \n",
        "      init='k-means++', \n",
        "      max_iter=100, \n",
        "      n_init=1)\n",
        "print(\"Clustering sparse data with %s\" % km)\n",
        "km.fit(X)\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d:\" % i)\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind])\n",
        "        "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
            "    n_clusters=12, n_init=1, n_jobs=None, precompute_distances='auto',\n",
            "    random_state=None, tol=0.0001, verbose=0)\n",
            "Top terms per cluster:\n",
            "Cluster 0:\n",
            " mustard\n",
            " dijon\n",
            " vinegar\n",
            " chopped\n",
            " oil\n",
            " olive\n",
            " pepper\n",
            " minced\n",
            " lemon\n",
            " black\n",
            "Cluster 1:\n",
            " chicken\n",
            " broth\n",
            " chopped\n",
            " lowsalt\n",
            " canned\n",
            " white\n",
            " dried\n",
            " oil\n",
            " butter\n",
            " garlic\n",
            "Cluster 2:\n",
            " olive\n",
            " oil\n",
            " garlic\n",
            " chopped\n",
            " extravirgin\n",
            " minced\n",
            " leaves\n",
            " pepper\n",
            " cloves\n",
            " dried\n",
            "Cluster 3:\n",
            " chocolate\n",
            " unsweetened\n",
            " bittersweet\n",
            " semisweet\n",
            " sugar\n",
            " cream\n",
            " vanilla\n",
            " chopped\n",
            " butter\n",
            " cocoa\n",
            "Cluster 4:\n",
            " chopped\n",
            " finely\n",
            " onion\n",
            " pepper\n",
            " oil\n",
            " ground\n",
            " garlic\n",
            " red\n",
            " olive\n",
            " diced\n",
            "Cluster 5:\n",
            " ground\n",
            " salt\n",
            " water\n",
            " butter\n",
            " pepper\n",
            " black\n",
            " sugar\n",
            " oil\n",
            " chopped\n",
            " flour\n",
            "Cluster 6:\n",
            " sugar\n",
            " cream\n",
            " vanilla\n",
            " egg\n",
            " heavy\n",
            " milk\n",
            " butter\n",
            " yolks\n",
            " whipping\n",
            " chilled\n",
            "Cluster 7:\n",
            " baking\n",
            " flour\n",
            " sugar\n",
            " butter\n",
            " salt\n",
            " soda\n",
            " vanilla\n",
            " eggs\n",
            " brown\n",
            " ground\n",
            "Cluster 8:\n",
            " sliced\n",
            " thinly\n",
            " red\n",
            " oil\n",
            " chopped\n",
            " olive\n",
            " pepper\n",
            " vinegar\n",
            " onion\n",
            " halved\n",
            "Cluster 9:\n",
            " cut\n",
            " pieces\n",
            " chopped\n",
            " slices\n",
            " inchthick\n",
            " potatoes\n",
            " medium\n",
            " oil\n",
            " olive\n",
            " crosswise\n",
            "Cluster 10:\n",
            " juice\n",
            " lemon\n",
            " orange\n",
            " sugar\n",
            " grated\n",
            " peel\n",
            " lime\n",
            " zest\n",
            " water\n",
            " finely\n",
            "Cluster 11:\n",
            " sauce\n",
            " soy\n",
            " sesame\n",
            " rice\n",
            " asian\n",
            " minced\n",
            " oil\n",
            " ginger\n",
            " garlic\n",
            " chopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIurt_ev7Llh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['label'] = km.labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KTILDk-7Lln",
        "colab_type": "text"
      },
      "source": [
        "Do some cleanup and save data as json file containing recipe title, rating, review count, publication date, % of reviewers who said they will make the recipe again, t-SNE coords, and cluster label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKge_fz-7Llo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "5f66797a-760e-4f99-b503-8c8fbaa98607"
      },
      "source": [
        "datestrings = []\n",
        "for index, row in data.iterrows():\n",
        "    pdate = row['pubDate']\n",
        "    data.at[index,'pubDate'] = time.mktime(datetime.strptime(pdate, '%Y-%m-%dT%H:%M:%S.%fZ').timetuple())/1e+09\n",
        "    datestrings.append(pd.to_datetime.pdate[0:10])\n",
        "data['datestr'] = datestrings"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3e75bfde4583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pubDate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pubDate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmktime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%dT%H:%M:%S.%fZ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimetuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e+09\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdatestrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datestr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatestrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be string, not float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMjhcapLI9Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKD4KJRJ7Lls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('epicurious_data.json','w') as f:\n",
        "    f.write(data.to_json(orient = \"records\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnbLSok5Icb_",
        "colab_type": "text"
      },
      "source": [
        "Dieses Datenset (epicurious_data.json) muss nun heruntergeladen werden und kann mit Processing visualisiert werden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0b_FVO7Llz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}